{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Examine Train/Test Sets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "outputs": [],
   "source": [
    "df_train_sets = pd.read_pickle('ebw_poc_train_sample_h1.pkl')\n",
    "df_test_sets  = pd.read_pickle('ebw_poc_test_sample_h1.pkl')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.concat([df_train_sets.groupby(['weeks_since_hit'])['target'].count(), df_test_sets.groupby(['weeks_since_hit'])['target'].count()], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_train_sets.groupby(['weeks_since_hit', 'censoring_flg_mod'])['target'].count()/df_train_sets.groupby(['weeks_since_hit'])['target'].count()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_extract_train = df_train_sets[df_train_sets.index.get_level_values(3) == 5]\n",
    "df_extract_test =  df_test_sets[df_test_sets.index.get_level_values(3) == 5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_extract_train.groupby(['weeks_since_hit', 'censoring_flg_mod'])['target'].count()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_extract_test.groupby(['weeks_since_hit', 'censoring_flg_mod'])['target'].count()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_extract_train['target'].plot(kind='hist')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "df_extract_train['target'].apply(lambda x: np.log(x)).plot(kind='hist')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "outputs": [],
   "source": [
    "# import sksurv and sklearn packages\n",
    "from sksurv.linear_model import CoxnetSurvivalAnalysis\n",
    "from sksurv.ensemble import GradientBoostingSurvivalAnalysis, RandomSurvivalForest\n",
    "from sksurv.metrics import as_concordance_index_ipcw_scorer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "import pickle"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### define prereqs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "outputs": [],
   "source": [
    "id_cols = list(df_train_sets.index)\n",
    "target_cols = ['censoring_flg_mod', 'target']\n",
    "cat_cols = ['market_name']\n",
    "feature_cols = list(set(df_train_sets.columns) - set(target_cols))\n",
    "num_cols = list(set(feature_cols) -set(cat_cols))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "split_number = 5\n",
    "random_state = 1000\n",
    "weeks_range = range(27)\n",
    "\n",
    "# hyperparam grid for ElasticNet Baseline\n",
    "param_grid_cnet = {\n",
    "    'estimator__l1_ratio': [0.5, 0.1, 0.01, 0.001]\n",
    "}\n",
    "\n",
    "# hyperparam grid for Gradient Boosted Model\n",
    "param_grid_gbm = {\n",
    "    'estimator__max_depth': [2, 4, 6, 8, 10],\n",
    "    'estimator__max_features': ['auto', 'sqrt', 'log2', None],\n",
    "    'estimator__n_estimators': [50, 100, 150, 200],\n",
    "    'estimator__learning_rate': [0.01, 0.001, 0.5, 0.1],\n",
    "    'estimator__loss': ['coxph'],\n",
    "    'estimator__subsample': [0.70, 0.80, 0.9, 1.0],\n",
    "    'estimator__min_samples_split': [2, 4, 8, 16],\n",
    "    'estimator__min_samples_leaf': [2, 4, 8, 16]\n",
    "}\n",
    "\n",
    "# hyperparam grid for Random Survival Forest\n",
    "param_grid_rsf = {\n",
    "    'estimator__max_depth': [2, 4, 6, 8, 10],\n",
    "    'estimator__max_features': ['auto', 'sqrt', 'log2', None],\n",
    "    'estimator__n_estimators': [50, 100, 150, 200],\n",
    "    'estimator__min_samples_split': [2, 4, 8, 16, 32],\n",
    "    'estimator__min_samples_leaf': [2, 4, 8, 16, 32],\n",
    "    'estimator__max_leaf_nodes': [2, 4, 8, 16, 32],\n",
    "    'estimator__bootstrap': [True, False],\n",
    "    'estimator__oob_score': [False, True],\n",
    "    'estimator__max_samples': [0.70, 0.80, 0.90, 1.0]\n",
    "}\n",
    "\n",
    "# best scores - train/test\n",
    "best_scores_cnet_train = {}\n",
    "best_scores_gbm_train = {}\n",
    "best_scores_rsf_train = {}\n",
    "\n",
    "best_scores_cnet_test = {}\n",
    "best_scores_gbm_test = {}\n",
    "best_scores_rsf_test = {}\n",
    "\n",
    "# best estimators\n",
    "best_estimators_cnet = {}\n",
    "best_estimators_gbm = {}\n",
    "best_estimators_rsf = {}\n",
    "\n",
    "# pickle file names\n",
    "file_cnet='best_est_cnet_h1_20230630.pkl'\n",
    "file_gbm='best_est_gbm_h1_20230630.pkl'\n",
    "file_rsf='best_est_rsf_h1_20230630.pkl'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### train a baseline ElasticNet model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for wk in weeks_range:\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    sv = CoxnetSurvivalAnalysis()\n",
    "\n",
    "    X_train = df_train_sets[df_train_sets.index.get_level_values(3) == wk][feature_cols]\n",
    "    X_train = pd.get_dummies(X_train, columns=['market_name'])\n",
    "    X_train[num_cols] = StandardScaler().fit_transform(X_train[num_cols])\n",
    "    y_train = df_train_sets[df_train_sets.index.get_level_values(3) == wk][target_cols].to_records(index=False)\n",
    "\n",
    "    sv_model = RandomizedSearchCV(estimator=as_concordance_index_ipcw_scorer(sv), param_distributions=param_grid_cnet,\n",
    "                                  cv=split_number, verbose=1, n_iter=50, n_jobs=-1)\n",
    "    sv_model.fit(X_train, y_train)\n",
    "    best_estimators_cnet[wk] = sv_model.best_estimator_\n",
    "    stop = time.perf_counter()\n",
    "    print(str(wk) + '_' + 'best_score: ' + f'{sv_model.best_score_}')\n",
    "    print('Elapsed time: '  + str(int((stop-start))))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# write best estimators to a pickle file\n",
    "with open(file_cnet, 'wb') as file:\n",
    "    pickle.dump(best_estimators_cnet, file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### train a GBM model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train a gbm\n",
    "for wk in weeks_range:\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    sv = GradientBoostingSurvivalAnalysis()\n",
    "\n",
    "    X_train = df_train_sets[df_train_sets.index.get_level_values(3) == wk][feature_cols]\n",
    "    X_train = pd.get_dummies(X_train, columns=['market_name'])\n",
    "    df_y_temp = df_train_sets[df_train_sets.index.get_level_values(3) == wk][target_cols]\n",
    "    y_train = df_y_temp.to_records(index=False)\n",
    "\n",
    "    # train gbm model\n",
    "    sv_model = RandomizedSearchCV(estimator=as_concordance_index_ipcw_scorer(sv), param_distributions=param_grid_gbm,\n",
    "                                  cv=split_number, verbose=1, n_iter=50, n_jobs=-1)\n",
    "    sv_model.fit(X_train, y_train)\n",
    "    best_estimators_gbm[wk] = sv_model.best_estimator_\n",
    "    stop = time.perf_counter()\n",
    "    print(str(wk) + '_' + 'best_score: ' + f'{sv_model.best_score_}')\n",
    "    print('Elapsed time: '  + str(int((stop-start))))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# write best estimators to a pickle file\n",
    "with open(file_gbm, 'wb') as file:\n",
    "    pickle.dump(best_estimators_gbm, file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### train a RandomSurvivalForest"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train a gbm\n",
    "for wk in weeks_range:\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    sv = RandomSurvivalForest()\n",
    "\n",
    "    X_train = df_train_sets[df_train_sets.index.get_level_values(3) == wk][feature_cols]\n",
    "    X_train = pd.get_dummies(X_train, columns=['market_name'])\n",
    "    df_y_temp = df_train_sets[df_train_sets.index.get_level_values(3) == wk][target_cols]\n",
    "    y_train = df_y_temp.to_records(index=False)\n",
    "\n",
    "    # train rsf model\n",
    "    sv_model = RandomizedSearchCV(estimator=as_concordance_index_ipcw_scorer(sv), param_distributions=param_grid_rsf,\n",
    "                                  cv=split_number, verbose=1, n_iter=50, n_jobs=-1)\n",
    "    sv_model.fit(X_train, y_train)\n",
    "    best_estimators_rsf[wk] = sv_model.best_estimator_\n",
    "    stop = time.perf_counter()\n",
    "    print(str(wk) + '_' + 'best_score: ' + f'{sv_model.best_score_}')\n",
    "    print('Elapsed time: '  + str(int((stop-start))))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# write best estimators to a pickle file\n",
    "with open(file_rsf, 'wb') as file:\n",
    "    pickle.dump(best_estimators_rsf, file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}